{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem Definition: The goal of this project is to create a sentiment analysis model that can classify movie reviews as either positive or negative based on their content. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas scikit-learn matplotlib seaborn nltk wordcloud textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from collections import Counter\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The read_reviews function will loop through the Imdb directory and aggregate the training and testing data into two lists.\n",
    "We collect our list of training reviews and corresponding sentiments, as well as the testing data, and load them into panda dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_reviews(data_dir):\n",
    "    reviews = []\n",
    "    sentiments = []\n",
    "\n",
    "    for sentiment in ['pos', 'neg']:\n",
    "        sentiment_dir = os.path.join(data_dir, sentiment)\n",
    "        for filename in os.listdir(sentiment_dir):\n",
    "            with open(os.path.join(sentiment_dir, filename), 'r', encoding='utf-8') as f:\n",
    "                reviews.append(f.read())\n",
    "                sentiments.append(sentiment)\n",
    "    return reviews, sentiments\n",
    "\n",
    "train_data_dir = 'data/aclImdb/train'\n",
    "train_reviews, train_sentiments = read_reviews(train_data_dir)\n",
    "\n",
    "test_data_dir = 'data/aclImdb/test'\n",
    "test_reviews, test_sentiments = read_reviews(test_data_dir)\n",
    "\n",
    "train_df = pd.DataFrame({'review': train_reviews, 'sentiment':train_sentiments})\n",
    "test_df = pd.DataFrame({'review': test_reviews, 'sentiment': test_sentiments})\n",
    "\n",
    "print(train_df.head())\n",
    "print(test_df.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our dataframes, it's time to explore the data. Unlike the data containing continuous numerical features in our wine and flower classifiers, we instead of text-based data.\n",
    "We will analyze the top-n most frequent words in both positive and negative reviews.\n",
    "So we must preprocess the text data by tokenizing, removing stop words, applying lemmatization\n",
    "Then we will create frequency distributions for both positive and negative reviews\n",
    "Finally we will be able to visualize the top-n most frequent words for both positive and negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "#Tokenize, remove stopwords, lemmatize input text\n",
    "def preprocess_text(text):\n",
    "    words = nltk.word_tokenize(text.lower())\n",
    "\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_words = []\n",
    "    for word in words:\n",
    "        if word.isalnum() and word not in stop_words:\n",
    "            filtered_words.append(word)\n",
    "    words = filtered_words\n",
    "\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_words = []\n",
    "    for word in words:\n",
    "        if abs(TextBlob(word).sentiment.polarity) > 0.4:\n",
    "            lemmatized_word = lemmatizer.lemmatize(word)\n",
    "            lemmatized_words.append(lemmatized_word)\n",
    "    words = lemmatized_words\n",
    "\n",
    "    return words\n",
    "\n",
    "positive_reviews = train_df[train_df['sentiment'] == 'pos']['review']\n",
    "negative_reviews = train_df[train_df['sentiment'] == 'neg']['review']\n",
    "\n",
    "positive_words = []\n",
    "for review in positive_reviews:\n",
    "    preprocessed_review = preprocess_text(review)\n",
    "    for word in preprocessed_review:\n",
    "        positive_words.append(word)\n",
    "\n",
    "negative_words = []\n",
    "for review in negative_reviews:\n",
    "    preprocessed_review = preprocess_text(review)\n",
    "    for word in preprocessed_review:\n",
    "        negative_words.append(word)\n",
    "\n",
    "positive_word_freq = Counter(positive_words)\n",
    "negative_word_freq = Counter(negative_words)\n",
    "\n",
    "#Visualize top-n most frequent words for positive and negative reviews with matplotlib\n",
    "def plot_word_freq(word_freq, n, title):\n",
    "    top_n_words = word_freq.most_common(n)\n",
    "    words, frequencies = zip(*top_n_words)\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.bar(words, frequencies)\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "n = 20  # amount of top words we want to visualize\n",
    "plot_word_freq(positive_word_freq, n, 'Most Frequent Words in Positive Reviews')\n",
    "plot_word_freq(negative_word_freq, n, 'Most Frequent Words in Negative Reviews')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the data preprocessed we can now vectorize the features into a numerical format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(tokenizer=preprocess_text, ngram_range=(1, 2))\n",
    "X_train = vectorizer.fit_transform(train_df['review'])\n",
    "X_test = vectorizer.transform(test_df['review'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
